{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It Together\n",
    "\n",
    "This program demonstrates how to programmatically retrieve data from the [Library of Congress API](https://libraryofcongress.github.io/data-exploration/index.html) and load that data into [Pandas](https://pandas.pydata.org/) for processing and aalysis. This program uses the [Requests]() Python library for making HTTP requests for the JSON representations of the [Selected Digitized Books](https://www.loc.gov/collections/selected-digitized-books/) collection. Using the APIs pagination features, the program grabs the first 5 pages of the collection with 50 items each for a total of 250 items. The program saves these index JSON files to disk and then proceeds to iterate through each result set downloading the JSON representation of all 250 items (and saving them to disk). Finally, some data is extracted from the item level JSON and put into a Pandas Dataframe for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory for saving files\n",
    "DATA_DIR = \"json-data/\"\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Depth parameter\n",
    "PAGE_LIMIT = 5\n",
    "\n",
    "\n",
    "# HTTP Parameters\n",
    "BASE_URL = \"https://loc.gov\"\n",
    "ENDPOINT = \"/collections/selected-digitized-books\"\n",
    "FORMAT = \"json\"\n",
    "RESULTS_PER_PAGE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Collection Index\n",
    "\n",
    "* This section fetches JSON representation of the collection and saves it to disk\n",
    "* It also shows how to do pagination by looping through a range from 1 to PAGE_LIMIT\n",
    "* Each iteration builds a URL for page based on the parameters set above and the current `page_num`\n",
    "* Save each index file to disk in the DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the first n index pages with a loop for each page\n",
    "for page_num in range(1,PAGE_LIMIT+1):\n",
    "    \n",
    "    # build a URL of the index page\n",
    "    URL = BASE_URL + ENDPOINT + \"/?fo={FORMAT}&c={RESULTS}&sp={PAGE}\".format(FORMAT=FORMAT,\n",
    "                                                                             RESULTS=RESULTS_PER_PAGE,\n",
    "                                                                             PAGE=page_num)\n",
    "    \n",
    "    # Fetch and parse the index JSON                                                                     \n",
    "    print(\"Fetching\", URL)\n",
    "    response = requests.get(URL)\n",
    "    collection_index = response.json()\n",
    "\n",
    "    # Save the json to disk\n",
    "    file_name = DATA_DIR + \"/index_\" + str(page_num) + \".json\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(collection_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Individual Items\n",
    "\n",
    "* Open the saved index files and extract just the results JSON\n",
    "* Using the results we can go fetch the actualy item-level JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a glob pattern to load the index files\n",
    "collection_indexes = list(Path(DATA_DIR).glob(\"index_*.json\"))\n",
    "collection_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use the `read_text()` function from Pathlib to very quickly suck the JSON data into memory\n",
    "* This is a shortcut around `with open(index,'r') as f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files into the JSON parser\n",
    "results_pile = [json.loads(index.read_text())['results'] for index in collection_indexes]\n",
    "# Check the length\n",
    "len(results_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_pile[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This isn't actually what we need, we want one list not a list of lists\n",
    "* We can't actually use a list comprehension here because we are *extending* the list\n",
    "* A list comprehension would give us a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold the result data\n",
    "results_pile = []\n",
    "# loop over each index file\n",
    "for file in collection_indexes:\n",
    "    # use pathlib to quickly read the file and parse as JSON\n",
    "    index = json.loads(file.read_text())\n",
    "    # append all results to the results_pile list\n",
    "    results_pile.extend(index['results'])\n",
    "        \n",
    "# Check the length to make sure everything is loaded\n",
    "len(results_pile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This loop iterates over every item, takes a URL, based on the item ID, and fetches the JSON representation\n",
    "* It saves the raw JSON to disk as a backup and also returns the JSON representation\n",
    "* Also sleeps for one second to prevent rate limiting with 250 items this loop should take 250 seconds or around 4 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for retreiving the item level JSON\n",
    "for counter, result in enumerate(results_pile):\n",
    "    \n",
    "    if counter % 50 == 0:\n",
    "        print(\"Fetching item \", counter)\n",
    "    \n",
    "    url = result['id']\n",
    "    # add JSON format request\n",
    "    url = url + \"?fo=json\"\n",
    "    # fetch the url\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # try parsing the response data and catch exceptions\n",
    "    try:\n",
    "        # parse json\n",
    "        item_json = response.json()\n",
    "    except:\n",
    "        # if we get an error display why and stop looping\n",
    "        print(response.status_code)\n",
    "        print(response.headers)\n",
    "        break\n",
    "        \n",
    "    # get the lccn so we can make a unique filename\n",
    "    lccn = item_json['item'][\"library_of_congress_control_number\"]\n",
    "    # generate a filename and write json to disk\n",
    "    filename = DATA_DIR + \"item_\" + lccn + \".json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(item_json, f)\n",
    "    \n",
    "    # Sleep for one second\n",
    "    sleep(1)\n",
    "    \n",
    "# Display a finished message\n",
    "print(\"Download Complete. Fetched {} items.\".format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Files\n",
    "\n",
    "* Now that we have saved the JSON files to disk we can open them from disk\n",
    "    * This is much faster than processing them from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a master list of all the item json files\n",
    "item_files = list(Path(DATA_DIR).glob(\"item_*.json\"))\n",
    "len(item_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that opens and parses the json files\n",
    "def open_item(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "# use a list comprehension to open all the JSON files\n",
    "item_pile = [open_item(path) for path in item_files]\n",
    "# how many files did we open\n",
    "len(item_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of fields we want to keep\n",
    "keys_of_interest = [\n",
    "    \"library_of_congress_control_number\",\n",
    "    \"date\",\n",
    "    \"title\",\n",
    "    \"medium\",\n",
    "    \"created_published\",\n",
    "    \"id\"]\n",
    "\n",
    "# Make a function that filters out just the fields we want\n",
    "def get_fields(item):\n",
    "    # use a dictionary comprehension to filter out just the fields we want\n",
    "    return {key : item['item'][key] for key in keys_of_interest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data into a dataframe\n",
    "# use a list comprehension with our filter function\n",
    "data = pd.DataFrame([get_fields(item) for item in item_pile])\n",
    "# display the first 100 items\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we are going to go some fancy stuff\n",
    "* The `medium` column has some interesting data, but it is currently an array of string values\n",
    "    * Since that is how the data came from the JSON response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just the medium colum\n",
    "data['medium']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first item to get the string values\n",
    "data['medium'].str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the string on spaces\n",
    "data['medium'].str.get(0).str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first item in the new list\n",
    "data['medium'].str.get(0).str.split().str.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have some numbers, but they are currently string\n",
    "* We also have some bad data here as well, so we will have to ignore those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the strings into numbers\n",
    "pd.to_numeric(data['medium'].str.get(0).str.split().str.get(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some of these won't convert because it is messy data, so lets just ignore those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the strings into numbers and ignore the onc \n",
    "book_lengths = pd.to_numeric(data['medium'].str.get(0).str.split().str.get(0),\n",
    "                            errors='coerce')\n",
    "book_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have some reasonably clean data in a tabular format we can start doing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics about book lengths\n",
    "book_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram of the book lenghts\n",
    "book_lengths.hist(bins=100, figsize=(10,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
